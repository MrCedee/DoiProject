{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29e5a043",
   "metadata": {},
   "source": [
    "# Práctica IBD\n",
    "\n",
    "* Matías Mussini\n",
    "* Antonio Reviejo\n",
    "* Pablo Alonso\n",
    "* Marcos Cedenilla\n",
    "\n",
    "Arquitectura para el análisis y almacenado de artículos científicos a partir de DOIs, proporcionados en un archivo txt, mediante la herramienta semantic scholar se extraerá un json con los datos de cada artículo, se irá almacenando por batches en una base de datos documental, MongoDB, se opta por una carga en batches debido a la posibilidad de saturar la memoria principal si se opta por obtener la información de todos los documentos y luego se vuelcan a la base de datos, en el caso de volúmenes de dato tipo big data, de la misma forma tanto las consultas como la carga información en otra base de datos se hará siempre por batches, el tamaño del batch deberá ser el máximo permitido sin saturarse la memoria principal del ordenador, spark trabaja con dicha memoria por lo que el sistema establecido se aplica también en las queries spark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bb82e8e",
   "metadata": {},
   "source": [
    "## Creación de los archivos requeridos y carga de los artículos en las bases de datos\n",
    "\n",
    "En primer lugar se crearán los archivos csv requeridos, junto con sus cabeceras, en adelante los archivos se modificarán para obtener el contenido deseado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a3bf2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importanción de dependencias\n",
    "import csv\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Ajuste del batch size\n",
    "batch_size = 10000\n",
    "\n",
    "\n",
    "data_file = open('data/Authors.csv', 'w')\n",
    "csv_writer = csv.writer(data_file)\n",
    "header = [\"Autor\",\"Publicaciones\"]\n",
    "csv_writer.writerow(header)\n",
    "data_file.close()\n",
    "\n",
    "data_file = open('data/Documents.csv', 'w')\n",
    "csv_writer = csv.writer(data_file) \n",
    "header = [\"paperId\",\"title\",\"publicationDate\" ]\n",
    "csv_writer.writerow(header)\n",
    "data_file.close()\n",
    "\n",
    "data_file = open('data/Keywords.csv', 'w')\n",
    "csv_writer = csv.writer(data_file) \n",
    "header = [\"word\", \"frequency\"]\n",
    "csv_writer.writerow(header)\n",
    "data_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ef80072",
   "metadata": {},
   "source": [
    "Volcado de los datos según se obtienen en la base de datos MongoDB, de esta manera al trabajar en batches se podría usar programación asíncrona, dado que mientras se buscan los siguentes datos se podría ir volcando los anteriores en MongoDB, lo que mejoraría la eficiencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a07e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "\n",
    "# Creación del cliente Mongo en el puerto correspondiente\n",
    "myclient = pymongo.MongoClient(\"mongodb://mongo1\")\n",
    "mydb = myclient[\"mydatabase\"]\n",
    "# Nos movemos a la colección donde irán los documentos\n",
    "mycol = mydb[\"publicaciones\"]\n",
    "# Contacto con la API de SemanticScholar\n",
    "sch = SemanticScholar()\n",
    "\n",
    "# Apertura del archivo\n",
    "with open('data/dois.txt') as f:\n",
    "    \n",
    "    # Lectura de este, se podría hacer por batches si fuera necesario\n",
    "    dois = f.readlines()\n",
    "    # Lista auxiliar donde se guardará cada batch de documentos antes de volcarlos en Mongo\n",
    "    j = []\n",
    "    \n",
    "    # Procesamos cada línea\n",
    "    for doi in dois:\n",
    "        doi = doi.strip()\n",
    "        \n",
    "        # Obtenemos la información de la API\n",
    "        paper = sch.get_paper(str(doi))\n",
    "        \n",
    "        # Diccionario auxiliar donde se guardará la información de cada documento\n",
    "        j1 = dict()\n",
    "        \n",
    "        # Resolvemos la ausencia de un campo en los documentos\n",
    "        if paper.paperId:\n",
    "            j1['paperId'] = paper.paperId\n",
    "        else:\n",
    "            j1['paperId'] = None\n",
    "        if paper.title:\n",
    "            j1['title'] = paper.title\n",
    "        else:\n",
    "            j1['title'] = None\n",
    "        \n",
    "        if paper.publicationDate:\n",
    "            j1['publicationDate'] = paper.publicationDate\n",
    "        else:\n",
    "            j1['publicationDate'] = None\n",
    "\n",
    "        if paper.year:\n",
    "            j1['year'] =  paper.year\n",
    "        else:\n",
    "            j1['year'] = None   \n",
    "        \n",
    "        if paper.authors:\n",
    "            # Procesamos autores de manera distinta, ya qe en caso de haberlos será una lista de documentos embedidos\n",
    "            authors_ = []\n",
    "            for i in paper.authors:\n",
    "                authors_.append({'authorId' :i.authorId, 'name':i.name})\n",
    "            j1['authors'] = authors_\n",
    "        else:\n",
    "            j1['authors'] = None\n",
    "\n",
    "        if paper.abstract:\n",
    "            j1['abstract'] = paper.abstract\n",
    "        else:\n",
    "            j1['abstract'] = None\n",
    "        \n",
    "        # Añadimos el documento procesado al batch\n",
    "        j.append(j1)\n",
    "        \n",
    "        # Si obtenemos los suficientes, volcamos\n",
    "        if len(j) == batch_size:\n",
    "            mycol.insert_many(j)\n",
    "            # Vaciamos el batch\n",
    "            j = []\n",
    "\n",
    "# Insertamos los documentos restantes\n",
    "if len(j) != 0:\n",
    "    mycol.insert_many(j)\n",
    "    j = []\n",
    "    \n",
    "# Cerramos el cliente\n",
    "myclient.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7babe545",
   "metadata": {},
   "source": [
    "Para capturar las relaciones entre Autores y resolver distintas queries, optamos por volcar los datos en Neo4j, debido a su potencia al trabajar con relaciones y nodos. Para ello vamos extrayendo los datos de Mongo a Neo4j por batches para no saturar la memoria, otra opción sería crear un archivo intermedio csv, con el que luego volcar la información en neo4j a priori sería mas eficiente en tiempo aunque menos en memoria, si se opta por nuestra opción y de nuevo se hace de manera asíncrona es posible recortar esa distancia en cuanto a complejidad temporal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dc02757",
   "metadata": {},
   "source": [
    "* La parte más costosa a nivel de memoria de los datos es guardar el abstract o si se quisiera el texto completo, por lo tanto en puntos como este que esos datos no se extraen de mongo,\n",
    "el batch_size puede ampliarse significativamente, puediendo igualar la cantidad de datos y por lo tanto ser exactamente igual a una carga total en memoria y posterior volcado a neo4j\n",
    "\n",
    "Cabe destacar que este script siguiente es para la inicialización, si se quieren añadir datos a neo4j, se deberán obtener solo los añadidos a mongo y recibirlos de una query, de forma que no se recorra toda la base de datos mongo tratando de insertarlos, aunque con la sentencia merge no deberían existir duplicidades, este comentario se hace a nivel de eficiencia del sistema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ae109f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "batch_size = 100000\n",
    "# Volvemos a abrir un cliente Mongo y nos desplazamos a nuestra colección\n",
    "myclient = pymongo.MongoClient(\"mongodb://mongo1\")\n",
    "mydb = myclient[\"mydatabase\"]\n",
    "mycol = mydb[\"publicaciones\"]\n",
    "\n",
    "# Variable auxiliar para la descarga de los datos de Mongo en batches\n",
    "skip = 0\n",
    "\n",
    "# Iniciamos un driver Neo4j, conectándonos al puerto correspondiente\n",
    "uri = \"bolt://neo4j\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# Ciclo de control\n",
    "while True:\n",
    "    \n",
    "    # Query que extrae los datos de MongoDB, preparados para su carga en Neo4j, se añade skip y limit para el uso de batches\n",
    "    authorPaper = list(mycol.aggregate([\n",
    "        { \"$unwind\": \"$authors\"},\n",
    "        { \"$project\": {\"_id\":0,  \"authors.authorId\":1, \"authors.name\":1, \"paperId\":1} },\n",
    "        { \"$skip\": skip},\n",
    "        { \"$limit\": batch_size}\n",
    "    ]))\n",
    "    \n",
    "    # Si se extraen documentos\n",
    "    if len(authorPaper) > 0:\n",
    "        \n",
    "        # Se abre el driver neo4j para cargar el batch\n",
    "        with driver.session() as session:\n",
    "            with session.begin_transaction() as tx:\n",
    "                \n",
    "                # Query de carga de los datos\n",
    "                query = '''\n",
    "                UNWIND $data AS entry\n",
    "                MERGE (a:Author {name: entry.authors.name, id: entry.authors.authorId})\n",
    "                MERGE (b:Paper {title: entry.paperId})\n",
    "                MERGE (a)-[:WROTE]->(b)\n",
    "                '''\n",
    "                tx.run(query, data=authorPaper)\n",
    "    \n",
    "    # Mientras que el batch no se más pequeño que el batch size y por lo tanto el limit no se ha satisfecho por ausencia de más datos, se continúa\n",
    "    if len(authorPaper) < batch_size:\n",
    "        break\n",
    "\n",
    "    # Incrementamos el valor skip para cargar el siguiente batch, sin repetir información\n",
    "    skip += batch_size\n",
    "\n",
    "# Cerramos el cliente mongo    \n",
    "myclient.close()\n",
    "\n",
    "# Para las queries posteriores es importante reflejar las relaciones de colaboración entre autores, desde mongo solo podemos obtener las de cada autor con cada paper, por lo que las creamos en neo4j\n",
    "query = \"\"\"MATCH (a1:Author)-[:WROTE]->(p: Paper)<-[:WROTE]-(a2:Author) with a1, a2, count(distinct p) as w \n",
    "   MERGE (a1)-[:COLABORA {weight: w}]-(a2)\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    result = list(result)\n",
    "    \n",
    "# Cerramos el driver neo4j\n",
    "driver.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c185d49",
   "metadata": {},
   "source": [
    "## Completando los archivos csv requeridos\n",
    "\n",
    "A continuación se mostrarán las lineas de código usadas para completar los csv pedidos en el guión de la práctica."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c9cfaa2",
   "metadata": {},
   "source": [
    "### Datos estáticos\n",
    "\n",
    "* De nuevo el abstract o texto no es necesario aquí por lo que podemos mantener un alto batch_size e idílicamente con buenos recursos un batch_size que permita cargar todos los datos, debido a que\n",
    "  son unos pocos datos str que no ocupan gran cantidad de espacio, aún así pensando en una posible cantidad ingente de artículos del orden de $10^7$ dejamos el sistema de batches, por si se\n",
    "  necesitase\n",
    "  \n",
    "De manera similar al comentario hecho en neo4j, estos scripts están pensados para inicialización y no actualización de los csv, se entienden que se ejecutarán una vez la base de datos esta comnpleta\n",
    "de no ser así bastaría con obtener de nuevo con una query mongo los datos nuevos e insertarlos con el mismo método, en contraste con recuperar la base de datos completa como se hace aquí,\n",
    "bastaría con una sentencía que filtrará por lo que es trivial.\n",
    "\n",
    "#### Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ad6a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myclient = pymongo.MongoClient(\"mongodb://mongo1\")\n",
    "mydb = myclient[\"mydatabase\"]\n",
    "mycol = mydb[\"publicaciones\"]\n",
    "\n",
    "data_file = open('data/Documents.csv', 'a')\n",
    "writer = csv.DictWriter(data_file, fieldnames=[\"paperId\",\"title\",\"publicationDate\" ])\n",
    "\n",
    "\n",
    "\n",
    "# Inicializamos el valor de skip\n",
    "skip = 0\n",
    "\n",
    "# Bucle de volcado en batches\n",
    "while True:\n",
    "    # Recuperamos un batch de documentos\n",
    "    documents = list(mycol.aggregate([\n",
    "        { \"$project\": {\"_id\":0, \"paperId\":1, \"publicationDate\":1, \"title\":1 }},\n",
    "        { \"$skip\": skip},\n",
    "        { \"$limit\": batch_size}\n",
    "    ]))\n",
    " \n",
    "    writer.writerows(documents)\n",
    "\n",
    "    # Chequeamos si hay más documentos\n",
    "    if len(documents) < batch_size:\n",
    "        break\n",
    "\n",
    "    # Incrementamos el valor skip para el siguiente batch\n",
    "    skip += batch_size\n",
    "\n",
    "# Cerramos el cliente mongo\n",
    "myclient.close()\n",
    "data_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4046e04b",
   "metadata": {},
   "source": [
    "#### Autores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2d76565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myclient = pymongo.MongoClient(\"mongodb://mongo1\")\n",
    "mydb = myclient[\"mydatabase\"]\n",
    "mycol = mydb[\"publicaciones\"]\n",
    "\n",
    "data_file = open('data/Authors.csv', 'a')\n",
    "writer = csv.DictWriter(data_file, fieldnames=[\"Autor\",\"Publicaciones\"])\n",
    "\n",
    "skip = 0\n",
    "while True:\n",
    "    \n",
    "    # Query para obtener los datos necesarios para el csv\n",
    "    authors = mycol.aggregate([\n",
    "        { \"$unwind\": \"$authors\"},\n",
    "        { \"$group\": {\"_id\": \"$authors.authorId\", \"Autor\": {\"$push\":\"$authors.name\"}, \"Publicaciones\" : {\"$sum\":1}}},\n",
    "        { \"$project\": {\"_id\":0, \"Autor\":1, \"Publicaciones\":1} },\n",
    "        { \"$unwind\": \"$Autor\"},\n",
    "        { \"$skip\": skip},\n",
    "        { \"$limit\": batch_size}\n",
    "    ])\n",
    "    \n",
    "    authors = list(authors)\n",
    "    writer.writerows(authors)\n",
    "        \n",
    "    if len(authors) < batch_size:\n",
    "        break\n",
    "    \n",
    "    skip += batch_size\n",
    "        \n",
    "myclient.close()\n",
    "data_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0335e04b",
   "metadata": {},
   "source": [
    "### Datos dinámicos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "800a3f60",
   "metadata": {},
   "source": [
    "Se ha optado por el uso de spark debido a su mayor eficiencia por el uso de memoria principal, lo cual queremos explotar al cargar la máxima cantidad de datos en esta para trabajar, su\n",
    "compatibilidad con python, lenguaje al que estamos más habituados, la descarga de datos de mongo se hará de la misma manera que anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28c48569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, count, udf, split, explode, col, transform, size\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql import SparkSession\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "# Función auxiliar para la creación de una sesión spark con los parámetros que necesitamos\n",
    "def init_spark_session():\n",
    "    spark = SparkSession.\\\n",
    "            builder.\\\n",
    "            appName(\"Cluster work\").\\\n",
    "            master(\"spark://spark-master:7077\").\\\n",
    "            config(\"spark.executor.memory\", \"512m\").\\\n",
    "            getOrCreate()\n",
    "    \n",
    "    return spark\n",
    "\n",
    "# Funciónes auxiliares para la limpieza del texto, eliminamos carácteres no deseados pero manteniendo los acentos\n",
    "def es_letra_con_tilde(caracter):\n",
    "    try:\n",
    "        return 'WITH' in unicodedata.name(caracter)\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    caracteres_validos = string.ascii_letters + string.digits\n",
    "    texto_limpiado = ''.join(c if c in caracteres_validos or es_letra_con_tilde(c) else '' for c in texto)\n",
    "    return texto_limpiado.lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eba4409c",
   "metadata": {},
   "source": [
    "#### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b38c67d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cuenta_palabras_spark(ruta: str, palabras: list, mycol):\n",
    "\n",
    "    # Las entradas serán la ruta del csv donde se volcarán los datos, la lista de palabras a buscar y la colección mongo de donde se obtendrán los documentos\n",
    "    \n",
    "    # Creamos una sesión spark\n",
    "    spark = init_spark_session()\n",
    "    \n",
    "    # Limpiamos las palabras\n",
    "    palabras = list(map(limpiar_texto, palabras))\n",
    "    \n",
    "    # Abrimos el archivo\n",
    "    data_file = open(ruta, 'a')\n",
    "    csv_writer = csv.writer(data_file)\n",
    "    \n",
    "    # Creamos una versión spark de la función anterior para poder usarla en un dataFrame spark\n",
    "    limpiar_texto_spark = udf(limpiar_texto)\n",
    "\n",
    "    # Inicializamos el valor de skip\n",
    "    skip = 0\n",
    "    \n",
    "    # Inicializamos a None el resultado final\n",
    "    final_result = None\n",
    "    \n",
    "    # Bucle de obtención de datos\n",
    "    while True:\n",
    "        \n",
    "        # Recuperamos un batch de documentos\n",
    "        projection = { 'abstract': 1, \"_id\": 0}\n",
    "        documents = list(mycol.find({}, projection).skip(skip).limit(batch_size))\n",
    "        \n",
    "        # No continuar si no se recuperan documentos\n",
    "        if len(documents) == 0:\n",
    "            break\n",
    "\n",
    "        # Convertimos el batch en un spark dataFrame\n",
    "        df = spark.createDataFrame(documents)\n",
    "\n",
    "        # Creamos una linea para cada palabra\n",
    "        df = df.select(explode(split(df.abstract, \" \")).alias(\"word\"))\n",
    "        \n",
    "        # Limpiamos el texto\n",
    "        df_with_processed_column = df.withColumn(\"word\", limpiar_texto_spark(df[\"word\"]))\n",
    "        \n",
    "        # Filtramos cada linea, si se encuentra en la lista de palabras la mantenemos si no se borrará\n",
    "        filtered_df = df_with_processed_column.filter(col(\"word\").isin(palabras))\n",
    "\n",
    "        # Contamos cada palabra\n",
    "        batch_result = filtered_df.groupBy(\"word\").count().alias(\"frequency\")\n",
    "\n",
    "        # Combinamos los resultados\n",
    "        if final_result is None:\n",
    "            final_result = batch_result\n",
    "        else:\n",
    "            # Unimos los dataFrames\n",
    "            final_result = final_result.union(batch_result)\n",
    "            # Sumamos los resultados\n",
    "            final_result = final_result.groupBy(\"word\").sum(\"frequency\")\n",
    "            # Renombramos la columna\n",
    "            final_result = final_result.withColumnRenamed(\"sum(frequency)\", \"frequency\")\n",
    "\n",
    "        # Chequeamos si hay más documentos que obtener\n",
    "        if len(documents) < batch_size:\n",
    "            break\n",
    "\n",
    "        # Incrementamos el valor skip para el siguiente batch\n",
    "        skip += batch_size\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "    # Escribimos el resultado final\n",
    "    csv_writer.writerows(final_result.collect())\n",
    "    \n",
    "    # Cerramos el archivo y la sesión\n",
    "    spark.stop()\n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1db2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/18 07:43:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# EJEMPLO\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://mongo1\")\n",
    "mydb = myclient[\"mydatabase\"]\n",
    "mycol = mydb[\"publicaciones\"]\n",
    "cuenta_palabras_spark('data/Keywords.csv', [\"the\", \"in\"], mycol)\n",
    "myclient.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae057ece",
   "metadata": {},
   "source": [
    "## Queries\n",
    "\n",
    "Con la infraestructura creada se debe ser capaz de resolver las siguientes consultas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fae975e",
   "metadata": {},
   "source": [
    "### Consultas simples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "324bf620",
   "metadata": {},
   "source": [
    "#### Articles\n",
    "\n",
    "Ya contamos con el json el cual tiene las obras y sus características. Todo esto guardado en Mongo. Por tanto, abriendo un nuevo cliente, obtenemos las obras de un autor ordenados por el número de autores que han participado en cada obra, dándole más imporancia a las obras en las que menos autores hayan participado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "742ab436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'titulo': ['Overview of Big Data and Its Visualization'], 'n_autores': 2}\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "\n",
    "\n",
    "def articles(name, mycol):\n",
    "    \n",
    "    Articles = list(mycol.aggregate([\n",
    "            { \"$unwind\": \"$authors\"},\n",
    "            { '$group': { '_id': '$paperId', 'nombre': { \"$push\": '$authors.name'},  'titulo': { \"$addToSet\": '$title'}, 'n_autores': { '$sum': 1 } } },\n",
    "            { \"$unwind\": \"$nombre\"},\n",
    "            { \"$match\": {\"nombre\": name}},\n",
    "            { \"$project\": {'_id': 0, 'titulo': 1, 'n_autores': 1}},\n",
    "            { \"$sort\": {'n_autores': 1}}\n",
    "        ]))\n",
    "\n",
    "    return Articles\n",
    "        \n",
    "myclient = pymongo.MongoClient(\"mongodb://mongo1\")\n",
    "mydb = myclient[\"mydatabase\"]\n",
    "mycol = mydb[\"publicaciones\"]\n",
    "name = 'G. Niu'\n",
    "result = articles(name, mycol)\n",
    "myclient.close()\n",
    "\n",
    "for i in result:\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01e3d607",
   "metadata": {},
   "source": [
    "#### Texts\n",
    "\n",
    "La consulta se hará mediante spark, siguiendo la estructura de la creación del archivo Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cafe0f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ADAPTAR TEXT A RECUPERAR LOS DATOS EN BATCH DESDE MONGO, LO HAGO YO, PONEROS MEJOR CON LAS OTRAS SUGERENCIAS QUE VAN ENTRE *** ***\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "def limpiar_texto_array(array):\n",
    "    return [limpiar_texto(d) for d in array]\n",
    "\n",
    "\n",
    "def texts(palabra, mycol):\n",
    "    \n",
    "    spark = init_spark_session()\n",
    "    \n",
    "    limpiar_texto_spark = udf(limpiar_texto_array, ArrayType(StringType()))\n",
    "\n",
    "    # Inicializamos el valor de skip\n",
    "    skip = 0\n",
    "    \n",
    "    # Inicializamos a None el resultado final\n",
    "    final_result = None\n",
    "    \n",
    "    # Bucle de obtención de datos\n",
    "    while True:\n",
    "        \n",
    "        # Recuperamos un batch de documentos\n",
    "        projection = { 'abstract': 1, \"title\": 1, \"_id\": 0}\n",
    "        documents = list(mycol.find({}, projection).skip(skip).limit(batch_size))\n",
    "        \n",
    "        # No continuar si no se recuperan documentos\n",
    "        if len(documents) == 0:\n",
    "            break\n",
    "\n",
    "        # Convertimos el batch en un spark dataFrame\n",
    "        df = spark.createDataFrame(documents)\n",
    "\n",
    "        df = df.select(\"title\", split(df.abstract, \" \").alias(\"word\"))\n",
    "        df = df.withColumn(\"word\", limpiar_texto_spark(col('word')))\n",
    "        df = df.withColumn('word_count', size(col('word')))\n",
    "        batch_result = df.withColumn(\"frequency\", size(pyspark.sql.functions.filter(col(\"word\"), lambda x: x == palabra))/col(\"word_count\")).select(\"title\", \"frequency\")\n",
    "        \n",
    "        # Combinamos los resultados\n",
    "        if final_result is None:\n",
    "            final_result = batch_result\n",
    "        else:\n",
    "            # Unimos los dataFrames\n",
    "            final_result = final_result.union(batch_result)\n",
    "\n",
    "        # Chequeamos si hay más documentos que obtener\n",
    "        if len(documents) < batch_size:\n",
    "            break\n",
    "\n",
    "        # Incrementamos el valor skip para el siguiente batch\n",
    "        skip += batch_size\n",
    "        \n",
    "        \n",
    "\n",
    "    final_result = final_result.sort(final_result.frequency.desc()).collect() # Para mayor legibilidad usar toPandas() aunque sería más costoso a nivel\n",
    "                                                                              # de memoria\n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16128500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  frequency\n",
      "0  Comparison between Expert Systems, Machine Lea...   0.090909\n",
      "1         Overview of Big Data and Its Visualization   0.044586\n"
     ]
    }
   ],
   "source": [
    "# EJEMPLO\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://mongo1\")\n",
    "mydb = myclient[\"mydatabase\"]\n",
    "mycol = mydb[\"publicaciones\"]\n",
    "palabra = \"the\"\n",
    "print(texts(palabra, mycol))\n",
    "myclient.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d9cb7cd",
   "metadata": {},
   "source": [
    "### Consultas Complejas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c03079ba",
   "metadata": {},
   "source": [
    "#### Collaborators\n",
    "\n",
    "Consulta realizada mediante neo4j debido a su potencia en el trabajo con relaciones. Debido a que la relación puede ser directa o indirecta. Por ello tendremos esto en cuenta a la hora de ordenar el listado que se devuelve, devolviendo primero los autores con los que se haya colaborado directamente, y dentro de ello, priorizando con quienes haya colaborado más veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a2c34a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autor: Y. Filali | Distancia: 1 | Peso: 1\n",
      "Autor: Karan Aggarwal | Distancia: 1 | Peso: 1\n",
      "Autor: Humam Al-Shahwani | Distancia: 1 | Peso: 1\n",
      "Autor: Dhamyaa Salim Mutar | Distancia: 1 | Peso: 1\n"
     ]
    }
   ],
   "source": [
    "def Colaborators(name, driver):\n",
    "    \n",
    "    name = '\\'' + name + '\\''\n",
    "    # Query\n",
    "    query = f'''\n",
    "            MATCH (r: Author {{name : {name} }})-[c:COLABORA*..2]-(r2:Author)\n",
    "            WHERE r <> r2\n",
    "            WITH DISTINCT r2\n",
    "            MATCH p = shortestPath((:Author {{name : {name} }})-[:COLABORA*..2]-(r2))\n",
    "            RETURN r2.name, length(p), reduce(totalWeight = 0, rel in relationships(p) | totalWeight + rel.weight) AS shortestPathWeight\n",
    "            ORDER BY length(p), shortestPathWeight\n",
    "            '''\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query)\n",
    "        result = list(result)\n",
    "\n",
    "    driver.close()\n",
    "        \n",
    "    return result\n",
    "        \n",
    "        \n",
    "# Creamos el driver Neo4j\n",
    "uri = \"bolt://neo4j\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "name = 'Maad M. Mijwil'\n",
    "result = Colaborators(name, driver)\n",
    "\n",
    "for i in result:\n",
    "    print(\"Autor:\", i[0], \"| Distancia:\", i[1], \"| Peso:\",i[2])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f77499a6",
   "metadata": {},
   "source": [
    "#### Words\n",
    "\n",
    "De nuevo consulta creada con spark, seguirá la misma estructura que las anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61c90d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def palabras_por_size(size: int, mycol):\n",
    "        \n",
    "    # Las entradas serán el tamaño requerido y la colección mongo\n",
    "    \n",
    "    # Filtrar para que el tamaño no pueda ser de más de 20 letras\n",
    "    if size < 1 or size > 20:\n",
    "        raise ValueError(\"size must be a integer between 1 and 20\")\n",
    "    \n",
    "    # Iniciamos la sesión\n",
    "    spark = init_spark_session()\n",
    "    limpiar_texto_spark = udf(limpiar_texto)\n",
    "\n",
    "    # Inicializamos el valor skip\n",
    "    skip = 0\n",
    "\n",
    "    final_result = None\n",
    "    \n",
    "    while True:\n",
    "        projection = { 'abstract': 1, \"_id\": 0}\n",
    "        # Recuperamos un batch de documentos, se podría recuperar solo el abstract\n",
    "        documents = list(mycol.find({}, projection).skip(skip).limit(batch_size))\n",
    "        \n",
    "        if len(documents) == 0:\n",
    "            break\n",
    "        \n",
    "        # Cramos el dataFrame en base al batch\n",
    "        df = spark.createDataFrame(documents)\n",
    "        \n",
    "        # Creamos una linea por palabra\n",
    "        df = df.select(explode(split(df.abstract, \" \")).alias(\"word\"))\n",
    "        \n",
    "        # Limpiamos el texto\n",
    "        df_with_processed_column = df.withColumn(\"word\", limpiar_texto_spark(df[\"word\"]))\n",
    "        \n",
    "        # Filtramos por tamaño\n",
    "        filtered_df = df_with_processed_column.filter(length(\"word\")==size)\n",
    "\n",
    "        # Contamos\n",
    "        batch_result = filtered_df.count()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # Combinamos los resultados\n",
    "        if final_result is None:\n",
    "            final_result = batch_result\n",
    "        else:\n",
    "            final_result =+ batch_result\n",
    "\n",
    "        # Chequeamos si hay más documentos\n",
    "        if len(documents) < batch_size:\n",
    "            break\n",
    "\n",
    "        # Incrementamos el skip para el siguiente batch\n",
    "        skip += batch_size\n",
    "\n",
    "    \n",
    "    # Cerramos la sesión y printeamos los resultados\n",
    "    spark.stop()\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5be5b4c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "# EJEMPLO\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://mongo1\")\n",
    "mydb = myclient[\"mydatabase\"]\n",
    "mycol = mydb[\"publicaciones\"]\n",
    "\n",
    "size = 3\n",
    "num = palabras_por_size(size, mycol)\n",
    "myclient.close()\n",
    "\n",
    "print(f'Existen: {num} de tamaño {size}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
